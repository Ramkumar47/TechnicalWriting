\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
% \usepackage{bm}
% \usepackage{setspace}

\renewcommand{\baselinestretch}{1.75}

\newcommand{\diag}[1]{diag({#1})}
\newcommand{\reshape}[2]{reshape \left({#1},\ {#2}\right)}
\newcommand{\bm}[1]{\textbf{#1}}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}

% \renewcommand{\algorithmiccomment}[1]{// #1}

\title{Recurrent Neural Network Algorithms}
\author{Ramkumar}

\begin{document}
\maketitle

\begin{algorithm}
    \renewcommand{\thealgorithm}{}
    \caption{RNN training}\label{alg:cap}
    \begin{algorithmic}
        \State load dataset $\{x_t,y_t\}, \ t=1,2,\dots,T$
        \State Perform encoding on $\{x_t\}$ \Comment{word embedding}
        \State Choose $N^{(1)}$ \Comment{hidden neurons count}
        \State Choose $\alpha$ \Comment{learning rate}
        \State initialize $a_0=\{0\}$, $\frac{\partial a_0}{\partial W} = \{0\}$\Comment{initializing 0\textsuperscript{th} hidden vector/tensor values}
        \State initialize $\frac{\partial a_0}{\partial U}=\{0\}$, $\frac{\partial a_0}{\partial b} = \{0\}$
        \State randomly initialize $W$,$U$,$b$,$V$ and $c$ \Comment{initializing model parameters}
        % \While{Not Converged}
        \Repeat
        % \State Perform Forward Propagation Procedure
        % \State Perform Back Propagation Through Time Procedure
        % \State Perform Parameter Update Procedure
        %
        \State Call \Call{Forward Propagation }{ }
        \State Call \Call{BPTT }{ }
        \State Call \Call{Parameter Update }{ }
        \Until{Convergence}
        % \Procedure{Forward Propagation}{}
        % \For{$t = 1,2,\ldots,T$}
        % \State $z_t = W a_{t-1} + U x_t + b$ \Comment{recurrent layer equations}
        % \State $a_t = h(z_t)$
        % \State $o_t = V a_t + c$\Comment{output layer equations}
        % \State $\hat{y}_t = softmax(o_t)$
        % \EndFor
        % \State Perform decoding on $\{\hat{y}_t\}$ \Comment{For validation}
        % \EndProcedure
        % \Procedure{Back Propagation Through Time}{}
        % \For{$t = 1,2,\ldots,T$}
        % \State Compute the following in the order
        % \begin{align*}
        %     \delta_t^{(2)} &= \diag{\hat{\bm{y}}_t - \bm{y}_t} \\
        %     \der{L_t}{\bm{c}} &= \delta_t^{(2)} \\
        %     \der{\bm{o}_t}{V} &= \reshape{\bm{a}_t^T\otimes \bm{I}_{N^{(2)}}}{N^{(2)}\times N^{(2)}\times N^{(1)}}\\
        %     \der{L_t}{V} &= \reshape{\delta_t^{(2)} \ \reshape{\der{\bm{o}_t}{\bm{V}}}{N^{(2)}\times\left(N^{(2)}*N^{(1)}\right)}}{N^{(2)}\times N^{(2)}\times N^{(1)}}\\
        %     \der{L_t}{\bm{a}_t} &= \delta_t^{(2)} \bm{V} \\
        %     \delta_t^{(1)} &= \der{L_t}{\bm{a}_t} \ \diag{h'(\bm{z}_t)} \\
        %     \der{L_t}{\bm{b}} &= \delta_t^{(1)} \left[\bm{I}_{N^{(1)}} + \bm{W} \der{\bm{a}_{t-1}}{\bm{b}}\right]\\
        %     \der{\bm{a}_t}{\bm{b}} &= \diag{h'(\bm{z}_t)} \\
        %     \der{\bm{W}\bm{a}_{t-1}}{\bm{W}} &= \reshape{\bm{a}_{t-1}^T \otimes \bm{I}_{N^{(1)}}}{N^{(1)}\times N^{(1)} \times N^{(1)}} \\
        %     \bm{W}\der{\bm{a}_{t-1}}{\bm{W}} &= \reshape{\bm{W} \ \reshape{\der{\bm{a}_{t-1}}{\bm{W}}}{N^{(1)}\times \left(N^{(1)}*N^{(1)}\right)}}{N^{(1)}\times N^{(1)}\times N^{(1)}} \\
        %     \omega_1 &= \der{\bm{W}\bm{a}_{t-1}}{\bm{W}}+\bm{W}\der{\bm{a}_{t-1}}{\bm{W}} \\
        %     \der{L_t}{\bm{W}} &= \reshape{\delta_t^{(1)} \ \reshape{\omega_1}{N^{(1)}\times \left(N^{(1)}*N^{(1)}\right)}}{N^{(2)}\times N^{(1)}\times N^{(1)}}\\
        %     \der{\bm{a}_t}{\bm{W}} &= \diag{h'\left(\bm{z}_t\right)} \omega_1 \\
        % \end{align*}
        % \EndFor
        % \EndProcedure
        % \EndWhile
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
%     \renewcommand{\thealgorithm}{}
    \begin{algorithmic}
        % \Procedure{Forward Propagation}{}
        % \For{$t = 1,2,\ldots,T$}
        % \State $z_t = W a_{t-1} + U x_t + b$ \Comment{recurrent layer equations}
        % \State $a_t = h(z_t)$
        % \State $o_t = V a_t + c$\Comment{output layer equations}
        % \State $\hat{y}_t = softmax(o_t)$
        % \EndFor
        % \State Perform decoding on $\{\hat{y}_t\}$ \Comment{For validation}
        % \EndProcedure
        \Function{Forward Propagation }{ }
        \For{$t = 1,2,\ldots,T$}
        \State $z_t = W a_{t-1} + U x_t + b$ \Comment{recurrent layer equations}
        \State $a_t = h(z_t)$
        \State $o_t = V a_t + c$\Comment{output layer equations}
        \State $\hat{y}_t = softmax(o_t)$
        \EndFor
        \State Perform decoding on $\{\hat{y}_t\}$ \Comment{For validation}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \renewcommand{\thealgorithm}{}
    \begin{algorithmic}
        % \Procedure{Back Propagation Through Time}{}
        \Function{BPTT}{ }\Comment{Back Propagation Through Time}
        \For{$t = 1,2,\ldots,T$}
        \State Compute the following in the order
        \begin{align*}
            \delta_t^{(2)} &= \diag{\hat{\bm{y}}_t - \bm{y}_t} \\
            \der{L_t}{\bm{c}} &= \delta_t^{(2)} \\
            \der{\bm{o}_t}{V} &= \reshape{\bm{a}_t^T\otimes \bm{I}_{N^{(2)}}}{N^{(2)}\times N^{(2)}\times N^{(1)}} \\
            \der{L_t}{V} &= \reshape{\delta_t^{(2)} \ \reshape{\der{\bm{o}_t}{\bm{V}}}{N^{(2)}\times\left(N^{(2)}*N^{(1)}\right)}}{N^{(2)}\times N^{(2)}\times N^{(1)}}\\
            \der{L_t}{\bm{a}_t} &= \delta_t^{(2)} \bm{V} \\
            \delta_t^{(1)} &= \der{L_t}{\bm{a}_t} \ \diag{h'(\bm{z}_t)} \\
            \der{L_t}{\bm{b}} &= \delta_t^{(1)} \left[\bm{I}_{N^{(1)}} + \bm{W} \der{\bm{a}_{t-1}}{\bm{b}}\right]\\
            \der{\bm{a}_t}{\bm{b}} &= \diag{h'(\bm{z}_t)} \\
            \der{\bm{W}\bm{a}_{t-1}}{\bm{W}} &= \reshape{\bm{a}_{t-1}^T \otimes \bm{I}_{N^{(1)}}}{N^{(1)}\times N^{(1)} \times N^{(1)}} \\
            \bm{W}\der{\bm{a}_{t-1}}{\bm{W}} &= \reshape{\bm{W} \ \reshape{\der{\bm{a}_{t-1}}{\bm{W}}}{N^{(1)}\times \left(N^{(1)}*N^{(1)}\right)}}{N^{(1)}\times N^{(1)}\times N^{(1)}} \\
            \omega_1 &= \der{\bm{W}\bm{a}_{t-1}}{\bm{W}}+\bm{W}\der{\bm{a}_{t-1}}{\bm{W}} \\
            \der{L_t}{\bm{W}} &= \reshape{\delta_t^{(1)} \ \reshape{\omega_1}{N^{(1)}\times \left(N^{(1)}*N^{(1)}\right)}}{N^{(2)}\times N^{(1)}\times N^{(1)}}\\
            \der{\bm{a}_t}{\bm{W}} &= \diag{h'\left(\bm{z}_t\right)} \omega_1 \\
            \der{\bm{U}\bm{x}_{t}}{\bm{U}} &= \reshape{\bm{x}_{t}^T \otimes \bm{I}_{N^{(1)}}}{N^{(1)}\times N^{(1)}\times n}\\
            \bm{W}\der{\bm{a}_{t-1}}{\bm{U}} &= \reshape{\bm{W} \ \reshape{\der{\bm{a}_{t-1}}{\bm{U}}}{N^{(1)}\times \left(N^{(1)}*n\right)}}{N^{(1)}\times N^{(1)}\times n} \\
            \omega_2 &= \der{\bm{U}\bm{x}_{t}}{\bm{U}}+\bm{W}\der{\bm{a}_{t-1}}{\bm{U}} \\
            \der{L_t}{\bm{U}} &= \reshape{\delta_t^{(1)} \ \reshape{\omega_2}{N^{(1)}\times \left(N^{(1)}*n\right)}}{N^{(2)}\times N^{(1)}\times n}\\
            \der{\bm{a}_{t}}{\bm{U}} &= \diag{h'(\bm{z}_t)} \omega_2
        \end{align*}
        \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \begin{algorithmic}
        \Function{Parameter Update}{ }
        \begin{align*}
            \bm{W}_{i,j} :&= \bm{W}_{i,j} - \alpha\sum_{t=1}^{T}\sum_{k=1}^{N^{(2)}} \left(\der{L_t}{\bm{W}}\right)_{k,i,j}, \ i=1,2,\dots,N^{(1)}, \ j=1,2,\dots,N^{(1)} \\
            \bm{U}_{i,j} :&= \bm{U}_{i,j} - \alpha\sum_{t=1}^{T}\sum_{k=1}^{N^{(2)}} \left(\der{L_t}{\bm{U}}\right)_{k,i,j}, \ i=1,2,\dots,N^{(1)}, \ j=1,2,\dots,n \\
            \bm{b}_i :&= \bm{b}_i - \alpha \sum_{t=1}^T \sum_{k=1}^{N^{(2)}} \left(\der{L_t}{\bm{b}}\right)_{k,i}, \ i=1,2,\dots,N^{(1)}\\
            \bm{V}_{i,j} :&= \bm{V}_{i,j} - \alpha\sum_{t=1}^{T}\sum_{k=1}^{N^{(2)}} \left(\der{L_t}{\bm{V}}\right)_{k,i,j}, \ i=1,2,\dots,N^{(2)}, \ j=1,2,\dots,N^{(1)} \\
            \bm{c}_i :&= \bm{c}_i - \alpha \sum_{t=1}^T \sum_{k=1}^{N^{(2)}} \left(\der{L_t}{\bm{c}}\right)_{k,i}, \ i=1,2,\dots,N^{(2)}
        \end{align*}
        \EndFunction
    \end{algorithmic}
\end{algorithm}
\end{document}
