[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Back Propagation Through Time for Recurrent Neural Networks",
    "section": "",
    "text": "Preface\nHere, I have derived the back propagation through time equations for the recurrent neural networks (RNN) as a part of my academics work. This derivation was made with an example RNN that has 2 input, 3 hidden neurons and 2 output.\nThe derivation was then extended to a general RNN having \\(n\\) input, \\(N^{(1)}\\) hidden neurons and \\(N^{(2)}\\) output. I concluded by specifying the pseudocode for RNN computation with single data having \\(T\\) subsamples, which is easy to extend for a dataset having multiple datapoints of different \\(T\\) values.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Recurrent Neural Networks (RNNs) are the type of neural networks used for mapping sequential data. It has same architecture of regular multi-layer perceptrons (MLPs), except that it will take additional components in its input along with data.\nRNNs require the sequential data to be split into sub-parts and then it will process each sub-part. The additional component that will be concatenated to the input vector of current sub-part is the hidden layer output vector that was obtained for the previous sub-part. By this way, the information about data sequence is learned by the network.\nHence, the back propagation equations will be different from the regular MLPs for these RNNs as they have to handle the information sequence for updating weights. Thus, it is called back propagation through time (BPTT).\nFor this derivation of BPTT, a simple RNN that as input vector size \\(n=2\\), output vector size \\(N^{(2)} = 2\\) and hidden neurons \\(N^{(1)} = 3\\) was taken as shown in Figure 1.1 .\n\n\n\n\n\n\nFigure 1.1: Recurrent Neural Network for BPTT derivation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "forwardPropagation.html",
    "href": "forwardPropagation.html",
    "title": "2  Forward Propagation in RNN",
    "section": "",
    "text": "2.1 Layer 1 equations\nFrom Figure 1.1, the output size of layer 1 is \\(N^{(1)} = 3\\) and input vector size \\(n=2\\).\nLet \\(\\textbf{z}_t=[z_{1,t}, z_{2,t},z_{3,t}]^T\\) be the pre-activation output vector. Then the forward propagation equations will be\n\\[\n\\begin{align}\nz_{1,t} &= w_{1,1} a_{1,t-1} + w_{1,2} a_{2,t-1} + w_{1,3} a_{3,t-1} + u_{1,1} x_{1,t} + u_{1,2} x_{2,t} + b_1\\\\\nz_{2,t} &= w_{2,1} a_{1,t-1} + w_{2,2} a_{2,t-1} + w_{2,3} a_{3,t-1} + u_{2,1} x_{1,t} + u_{2,2} x_{2,t} + b_2\\\\\nz_{3,t} &= w_{3,1} a_{1,t-1} + w_{3,2} a_{2,t-1} + w_{3,3} a_{3,t-1} + u_{3,1} x_{1,t} + u_{3,2} x_{2,t} + b_3\\\\\n\\end{align}\n\\tag{2.1}\\]\nIn vector form, it will be writen as\n\\[\n\\textbf{z}_t = \\textbf{W} \\textbf{a}_{t-1} + \\textbf{U} \\textbf{x}_t + \\textbf{b}\n\\tag{2.2}\\]\nwhere, \\(\\textbf{x}_t = [x_{1,t}, x_{2,t}]^T\\) and \\(\\textbf{a}_{t-1} =\n[a_{1,t-1},a_{2,t-1},a{3,t-1}]^T\\).\n\\(\\textbf{W} \\in \\mathbb{R}^{N^{(1)}\\times N^{(1)}}\\), \\(\\textbf{U} \\in\n\\mathbb{R}^{N^{(1)}\\times n}\\) and \\(\\textbf{b} \\in \\mathbb{R}^{N^{(1)}\\times 1}\\) are the layer parameters.\nNow, the Equation 2.2 will be passed through the activation function \\(h(.)\\) to induce non-linearity and the final output obtained from this layer will be\n\\[\n\\textbf{a}_t = h\\left(\\textbf{z}_t\\right) .\n\\tag{2.3}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forward Propagation in RNN</span>"
    ]
  },
  {
    "objectID": "forwardPropagation.html#layer-2-equations",
    "href": "forwardPropagation.html#layer-2-equations",
    "title": "2  Forward Propagation in RNN",
    "section": "2.2 Layer 2 equations",
    "text": "2.2 Layer 2 equations\nEquation 2.3 gives the output vector of previous layer, which will be taken as the input vector to the current layer as it happens in regular neural networks.\nLet, \\(\\textbf{o}_t \\in \\mathbb{R}^{N^{(2)}}\\) be the pre-activation output vector for the current layer. Then the forward propagation equation will be\n\\[\n\\textbf{o}_t = \\textbf{V} \\textbf{a}_t + \\textbf{c}\n\\tag{2.4}\\]\nHere, \\(\\textbf{V}\\in\\mathbb{R}^{N^{(2)}\\times N^{(1)}}\\) and \\(\\textbf{c}\\in\\mathbb{R}^{N^{(2)}\\times 1}\\) are the layer parameters.\nThen, the activation function will be applied to \\(\\textbf{o}_t\\). Here, for this derivation, the activation function is assumed to be softmax as the primary motive of this work is to develop RNN from scratch for autocompletion of words. So, the input and output will be a word-embedding vector of corresponding characters in the words.\nThe final output vector of current layer will be\n\\[\n\\hat{\\textbf{y}}_t = softmax(\\textbf{o}_t)\n\\tag{2.5}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forward Propagation in RNN</span>"
    ]
  },
  {
    "objectID": "forwardPropagation.html#loss-calculation",
    "href": "forwardPropagation.html#loss-calculation",
    "title": "2  Forward Propagation in RNN",
    "section": "2.3 Loss calculation",
    "text": "2.3 Loss calculation\nFor the case of using softmax activation (which is used for multiclass classification problem), the apropriate loss function will be categorical cross entropy\n\\[\nL_t = -\\textbf{y}_t^T \\log(\\hat{\\textbf{y}}_t).\n\\tag{2.6}\\]\nIt can be noted in Equation 2.6 that the loss is calculated sequence-wise, i.e. 1 loss per \\(t\\). So, the total loss will be\n\\[\nL = \\sum_{t=1}^T L_t.\n\\tag{2.7}\\]\nNext, we will see how to calculate the derivatives of Equation 2.6 with respect to all the RNN model parameters: \\(\\textbf{W},\\textbf{U},\\textbf{b},\\textbf{V}\\) and \\(\\textbf{c}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forward Propagation in RNN</span>"
    ]
  },
  {
    "objectID": "backPropagation.html",
    "href": "backPropagation.html",
    "title": "3  Back Propagation Through Time",
    "section": "",
    "text": "3.1 Computing loss gradients of layer 2 parameters\nNow, to compute loss gradient with respect to \\(\\textbf{c}\\) by taking Equation 2.4\n\\[\n\\begin{align}\n\\frac{\\partial L_t}{\\partial \\textbf{c}} &= \\frac{\\partial L_t}{\\partial \\textbf{o}_t} \\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{c}} \\\\\n&= \\delta_t^{(2)}\n\\begin{bmatrix}\n\\frac{\\partial o_{1,t}}{\\partial c_1} & \\frac{\\partial o_{1,t}}{\\partial c_2} \\\\\n\\frac{\\partial o_{2,t}}{\\partial c_1} & \\frac{\\partial o_{2,t}}{\\partial c_2} \\\\\n\\end{bmatrix}\n= \\delta_t^{(2)} \\textbf{I}_{N^{(2)}} = \\delta_t^{(2)}\n\\end{align}\n\\tag{3.3}\\]\nIt should be noted that in Equation 3.3, the \\(\\frac{\\partial \\textbf{o}_t}{\\partial\n\\textbf{c}}\\) is a vector-to-vector gradient resulting in a matrix as shown in the above steps.\nThen in similar way, computing loss gradient with respect to \\(\\textbf{V}\\) by taking the same Equation 2.4 as\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{V}} = \\frac{\\partial L_t}{\\partial \\textbf{o}_t} \\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}}\n\\tag{3.4}\\]\nIn the above equation, we need to compute \\(\\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}}\\) which is a vector-to-matrix gradient resulting in a 3D tensor as follows.\n\\[\n\\renewcommand{\\reshape}[2]{reshape \\left({#1},\\ {#2}\\right)}\n\\begin{align}\n\\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}} &= \\left[\\left[\\frac{\\partial o_{1,t}}{\\partial \\textbf{V}}\\right]\\left[ \\frac{\\partial o_{2,t}}{\\partial \\textbf{V}}\\right]\\right] \\\\\n&= \\left[\n\\begin{bmatrix}\na_{1,t} & a_{2,t} & a_{3,t} \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\na_{1,t} & a_{2,t} & a_{3,t}\n\\end{bmatrix}\n\\right] \\\\\n&= \\reshape{\\textbf{a}_t^T \\otimes \\textbf{I}_{N^{(2)}}}{ N^{(2)}\\times N^{(2)}\\times  N^{(1)}}\n\\end{align}\n\\tag{3.5}\\]\nSubstituting Equation 3.5 in Equation 3.4 will yield \\[\n\\frac{\\partial L_t}{\\partial \\textbf{V}} = \\delta_t^{(2)} \\reshape{\\textbf{a}_t^T \\otimes \\textbf{I}_{N^{(2)}}}{N^{(2)}\\times N^{(2)}\\times  N^{(1)}}.\n\\]\nHere, \\(\\otimes\\) is the Kronecker product that gives \\(\\textbf{a}_t^T \\otimes \\textbf{I}_{N^{(2)}}\\) as a 2D matrix of \\(N^{(2)}\\times(N^{(2)}*N^{(1)})\\) dimension. And \\(\\reshape{}{}\\) is an tensor reshaping operator that transforms the 2D matrix of shape \\(N^{(2)}\\times(N^{(2)}*N^{(1)})\\) into a 3D Tensor of shape \\(N^{(2)}\\times N^{(2)}\\times N^{(1)}\\).\nBut, \\(\\delta_t^{(2)}\\) is a matrix of shape \\(N^{(2)}\\times N^{(2)}\\) and the other operand is a 3D tensor in the above equation. So, again \\(\\reshape{}{}\\) operation is performed to convert the tensor into matrix, perform multiplication and then again convert back the resulting matrix into tensor as shown below.\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{V}} = \\reshape{\\delta_t^{(2)} \\reshape{\\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}}}{N^{(2)}\\times (N^{(2)}* N^{(1)})}}{ N^{(2)}\\times N^{(2)}\\times N^{(1)}}\n\\tag{3.6}\\]\nThis concludes the gradients derivation for layer 2. Next, similar steps will be performed for gradeient computation for layer 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "backPropagation.html#computing-loss-gradients-of-layer-2-parameters",
    "href": "backPropagation.html#computing-loss-gradients-of-layer-2-parameters",
    "title": "3  Back Propagation Through Time",
    "section": "",
    "text": "Note\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{c}}\\) is a matrix of dimension \\(N^{(2)}\\times N^{(2)}\\), with 1st dimension being number of components in the loss term.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{V}}\\) is a 3D tensor of dimension \\(N^{(2)}\\times N^{(2)} \\times N^{(1)}\\), with 1st dimension being number of components in the loss term.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "backPropagation.html#computing-loss-gradients-of-layer-1-parameters",
    "href": "backPropagation.html#computing-loss-gradients-of-layer-1-parameters",
    "title": "3  Back Propagation Through Time",
    "section": "3.2 Computing loss gradients of layer 1 parameters",
    "text": "3.2 Computing loss gradients of layer 1 parameters\nTo start with layer 1 parameters, we need a couple of intermediate derivatives that are computed below.\n\\[\n\\begin{align}\n\\frac{\\partial L_t}{\\partial \\textbf{a}_t} &= \\frac{\\partial L_t}{\\partial \\textbf{o}_t} \\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{a}_t} \\\\\n&= \\delta_t^{(2)}\n\\begin{bmatrix}\n\\frac{\\partial o_{1,t}}{\\partial a_{1,t}} & \\frac{\\partial o_{1,t}}{\\partial a_{2,t}} & \\frac{\\partial o_{1,t}}{\\partial a_{3,t}} \\\\\n\\frac{\\partial o_{2,t}}{\\partial a_{1,t}} & \\frac{\\partial o_{2,t}}{\\partial a_{2,t}} & \\frac{\\partial o_{2,t}}{\\partial a_{3,t}} \\\\\n\\end{bmatrix} \\\\\n&= \\delta_t^{(2)}\n\\begin{bmatrix}\nv_{1,1} & v_{1,2} & v_{1,3} \\\\\nv_{2,1} & v_{2,2} & v_{3,3} \\\\\n\\end{bmatrix} \\\\\n&= \\delta_T^{(2)} \\textbf{V}\n\\end{align}\n\\tag{3.7}\\]\nAnd\n\\[\n\\begin{align}\n\\frac{\\partial L_t}{\\partial \\textbf{z}_t} &= \\frac{\\partial L_t}{\\partial \\textbf{a}_t} \\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{z}_t} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t}\n\\begin{bmatrix}\n\\frac{\\partial a_{1,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{1,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{1,t}}{\\partial z_{3,t}} \\\\\n\\frac{\\partial a_{2,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{2,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{2,t}}{\\partial z_{3,t}} \\\\\n\\frac{\\partial a_{3,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{3,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{3,t}}{\\partial z_{3,t}} \\\\\n\\end{bmatrix} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t}\n\\begin{bmatrix}\nh'(z_{1,t}) & 0 & 0 \\\\\n0 & h'(z_{1,t}) & 0 \\\\\n0 & 0 & h'(z_{1,t}) \\\\\n\\end{bmatrix} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t} \\diag{h'\\left(\\textbf{z}_t\\right)} \\\\\n&= \\delta_t^{(1)}\n\\end{align}\n\\tag{3.8}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\delta_t^{(1)}\\) is a matrix of size \\(N^{(2)}\\times N^{(1)}\\).\n\n\nNow, we will start with the \\(\\textbf{b}\\) parameter’s loss gradient.\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{b}} = \\frac{\\partial L_t}{\\partial \\textbf{z}_t} \\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}}\n\\tag{3.9}\\]\nIt is evident from Equation 2.2 and Equation 2.3 that \\(\\textbf{a}_t\\) is dependent on \\(\\textbf{z}_t\\). However, \\(\\textbf{z}_t\\) is dependent upon \\(\\textbf{a}_{t-1}\\), and the chain goes on till \\(t=0\\). Let me call this as sequence chain link. Because of this, the \\(\\textbf{z}_t\\) derivative in above equation will be written as \\[\n\\begin{align}\n\\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}} &= \\frac{\\partial \\textbf{b}}{\\partial \\textbf{b}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}} \\\\\n&= \\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}} \\\\\n\\end{align}\n\\]\nWe will keep \\(\\partial\\textbf{a}_{t-1}/\\partial\\textbf{b}\\) as it is for now. Substituting above expression in Equation 3.9 gives\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{b}} = \\delta_t^{(1)} \\left[\\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}}\\right]. \\\\\n\\tag{3.10}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{b}}\\) is a vector of shape \\(N^{(2)}\\times N^{(1)}\\).\n\n\nNext, we compute \\[\n\\begin{align}\n\\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{b}} &= \\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{z}_t} \\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}} \\\\\n&= \\diag{h'(\\textbf{z}_t)} \\textbf{I}_{N^{(1)}} \\\\\n&= \\diag{h'(\\textbf{z}_t)}\n\\end{align}\n\\tag{3.11}\\]\nwhich is a \\(N^{(1)}\\times N^{(1)}\\) matrix.\nNow, we move to compute \\(\\textbf{W}\\) parameter’s loss gradient. From Equation 2.2, \\[\n\\renewcommand{\\bm}[1]{\\textbf{#1}}\n\\renewcommand{\\der}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\] \\[\n\\begin{align}\n\\der{L_t}{\\bm{W}} &= \\der{L_t}{\\bm{z}_t}\\der{\\bm{z}_t}{\\bm{W}} \\\\\n&= \\delta_t^{(1)}\\left[\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}} + \\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{W}}\\right] \\\\\n&= \\delta_t^{(1)} \\omega_1.\n\\end{align}\n\\tag{3.12}\\]\nLets compute each term in \\(\\omega_1\\) in above equation. Let \\[\n\\lambda = \\bm{W} \\bm{a}_{t-1} =\n\\begin{bmatrix}\nw_{1,1} a_{1,t-1} + w_{1,2}a_{2,t-1} + w_{1,3} a_{3,t-1} \\\\\nw_{2,1} a_{1,t-1} + w_{2,2}a_{2,t-1} + w_{2,3} a_{3,t-1} \\\\\nw_{3,1} a_{1,t-1} + w_{3,2}a_{2,t-1} + w_{3,3} a_{3,t-1} \\\\\n\\end{bmatrix}\n\\]\nNow, \\[\n\\begin{align}\n\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}} = \\der{\\lambda}{\\bm{W}} &=\n\\left[\\left[\\der{\\lambda_1}{\\bm{W}}\\right]\\left[\\der{\\lambda_2}{\\bm{W}}\\right]\\left[\\der{\\lambda_3}{\\bm{W}}\\right]\\right] \\\\\n&= \\left[\n\\begin{bmatrix}\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n\\end{bmatrix}\n\\right] \\\\\n\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}}&= \\reshape{\\bm{a}_{t-1}^T\\otimes \\bm{I}_{N^{(1)}}}{N^{(1)}\\times N^{(1)}\\times N^{(1)}} .\n\\end{align}\n\\tag{3.13}\\]\nWe keep \\(\\der{\\bm{a}_{t-1}}{\\bm{W}}\\) as it is for now in Equation 3.12 like previously did. Then,\n\\[\n\\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{W}} = \\reshape{\\bm{W}\\  \\reshape{\\der{\\bm{a}_{t-1}}{\\bm{W}}}{N^{(1)}\\times (N^{(1)}* N^{(1)})}}{N^{(1)}\\times N^{(1)}\\times N^{(1)}}.\n\\tag{3.14}\\]\nSubstituting Equation 3.13 and Equation 3.14 in Equation 3.12 can compute \\(\\omega_1\\) term. Thus, \\[\n\\der{L_t}{\\bm{W}} = \\reshape{\\delta_t^{(1)} \\ \\reshape{\\omega_1}{N^{(1)}\\times (N^{(1)}*N^{(1)})}}{N^{(2)}\\times N^{(1)} \\times N^{(1)}}.\n\\tag{3.15}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\der{L_t}{\\bm{W}}\\) is a 3D tensor of shape \\(N^{(2)}\\times N^{(1)} \\times N^{(1)}\\).\n\n\nNow, computing the last parameter \\(\\bm{U}\\) derivative of loss function\n\\[\n\\begin{align}\n\\der{L_t}{\\bm{U}} &= \\der{L_t}{\\bm{z}_t}\\der{\\bm{z}_t}{\\bm{U}} \\\\\n&= \\delta_t^{(1)} \\left[\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} + \\bm{W} \\der{\\bm{a}_{t-1}}{\\bm{U}}\\right]\\\\\n&= \\delta_t^{(2)} \\omega_2.\n\\end{align}\n\\tag{3.16}\\]\nNow to compute \\(\\omega_2\\) in above equation. Let \\[\n\\psi = \\bm{U}\\bm{x}_t =\n\\begin{bmatrix}\nu_{1,1} x_{1,t} + u_{1,2} x_{2,t} \\\\\nu_{2,1} x_{1,t} + u_{2,2} x_{2,t} \\\\\nu_{3,1} x_{1,t} + u_{3,2} x_{2,t} \\\\\n\\end{bmatrix}.\n\\]\nThen\n\\[\n\\begin{align}\n\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} = \\der{\\psi}{\\bm{U}} &=\n\\left[\\left[\\der{\\psi_1}{\\bm{U}}\\right]\\left[\\der{\\psi_2}{\\bm{U}}\\right]\\left[\\der{\\psi_3}{\\bm{U}}\\right]\\right] \\\\\n&= \\left[\n\\begin{bmatrix}\nx_{1,t} & x_{2,t} \\\\\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\nx_{1,t} & x_{2,t} \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\nx_{1,t} & x_{2,t} \\\\\n\\end{bmatrix}\n\\right] \\\\\n\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} &= \\reshape{\\bm{x}_t^T \\otimes \\bm{I}_{N^{(1)}}}{N^{(1)}\\times N^{(1)} \\times n} .\n\\end{align}\n\\tag{3.17}\\]\nNow, in the other term, we will keep \\(\\der{\\bm{a}_{t-1}}{\\bm{U}}\\) as it is for now like previously did. Then, \\[\n\\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{U}} = \\reshape{\\bm{W} \\reshape{\\der{\\bm{a}_{t-1}}{\\bm{U}}}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(1)}\\times N^{(1)}\\times n}\n\\tag{3.18}\\]\nNow, by substituting Equation 3.17 and Equation 3.18 in Equation 3.16, we can compute \\(\\omega_2\\). Thus,\n\\[\n\\der{L_t}{\\bm{U}} = \\reshape{\\delta_t^{(1)} \\reshape{\\omega_2}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(2)}\\times N^{(1)} \\times n}\n\\tag{3.19}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\der{L_t}{\\bm{U}}\\) is a 3D tensor of size \\(N^{(2)}\\times N^{(1)} \\times n\\).\n\n\nThis concludes the loss equation derivatives computation for all model parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "backPropagation.html#summary-of-loss-gradients",
    "href": "backPropagation.html#summary-of-loss-gradients",
    "title": "3  Back Propagation Through Time",
    "section": "3.3 Summary of loss gradients",
    "text": "3.3 Summary of loss gradients\nBelow is the summary of the loss gradient equations for all parameters.\n\\[\n\\begin{align}\n\\der{L_t}{\\bm{c}} &= \\delta_t^{(2)}\\\\\n\\frac{\\partial L_t}{\\partial \\textbf{V}} &= \\reshape{\\delta_t^{(2)} \\ \\reshape{\\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}}}{N^{(2)}\\times (N^{(2)}* N^{(1)})}}{ N^{(2)}\\times N^{(2)}\\times N^{(1)}} \\\\\n\\frac{\\partial L_t}{\\partial \\textbf{b}} &= \\delta_t^{(1)} \\left[\\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}}\\right] \\\\\n\\der{L_t}{\\bm{W}} &= \\reshape{\\delta_t^{(1)} \\ \\reshape{\\omega_1}{N^{(1)}\\times (N^{(1)}*N^{(1)})}}{N^{(2)}\\times N^{(1)} \\times N^{(1)}} \\\\\n\\der{L_t}{\\bm{U}} &= \\reshape{\\delta_t^{(1)} \\reshape{\\omega_2}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(2)}\\times N^{(1)} \\times n}\n\\end{align}\n\\]\nIn the next section, we will discuss about updating model parameters and the pseudocode for RNN training.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html",
    "href": "weightUpdate.html",
    "title": "4  Updating parameters and RNN training",
    "section": "",
    "text": "4.1 Updating parameters\nIn the last section, we have derived the loss gradients for all model parameters. It can be noted that the loss gradients have an extra dimension than its corresponding model parameters.\n\\[\n\\renewcommand{\\bm}[1]{\\textbf{#1}}\n\\renewcommand{\\der}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\]\nFor example,\nthe dimension of \\(\\der{L_t}{\\bm{U}}\\) is \\(\\left(N^{(2)}\\times\nN^{(1)}\\times n\\right)\\) whereas the dimension of \\(\\bm{U}\\) is \\(\\left(N^{(1)}\\times n\\right)\\).\nThis leading dimension \\(N^{(2)}\\) indicates the number of components in the final output vector. Thus, every parameter has a loss gradient derived from every output vector component. So, the loss gradients will be summed on the first axis (loss axis) to accumulate the update from all loss components and then used to update the parameters.\nHence, the update from each loss component will be accumulated by adding the loss gradients on the first axis and then used to update the parameters.\nThe parameters update equations are given below. The summation is performed on the loss axis direction.\n\\[\n\\begin{align}\n\\bm{c}_i :&= \\left. \\bm{c}_i - \\alpha \\sum_{t=1}^{N^{(2)}} \\sum_{k=1}^{N^{(2)}} \\der{L_t}{\\bm{c}}\\right|_{k,i}, \\ \\ i=1,2,\\dots,N^{(2)} \\\\\n\\bm{V}_{i,j} :&= \\left. \\bm{V}_{i,j} - \\alpha \\sum_{t=1}^{N^{(2)}}\\sum_{k=1}^{N^{(2)}} \\der{L_t}{\\bm{V}}\\right|_{k,i,j}, \\ \\ i=1,2,\\dots,N^{(2)}, \\ \\ j=1,2,\\dots,N^{(1)} \\\\\n\\bm{b}_i :&= \\left. \\bm{b}_i - \\alpha \\sum_{t=1}^{N^{(2)}}\\sum_{k=1}^{N^{(2)}} \\der{L_t}{\\bm{b}}\\right|_{k,i}, \\ \\ i=1,2,\\dots,N^{(1)} \\\\\n\\bm{W}_{i,j} :&= \\left. \\bm{W}_{i,j} - \\alpha \\sum_{t=1}^{N^{(2)}}\\sum_{k=1}^{N^{(2)}} \\der{L_t}{\\bm{W}}\\right|_{k,i,j}, \\ \\ i=1,2,\\dots,N^{(1)}, \\ \\ j=1,2,\\dots,N^{(1)} \\\\\n\\bm{U}_{i,j} :&= \\left. \\bm{U}_{i,j} - \\alpha \\sum_{t=1}^{N^{(2)}}\\sum_{k=1}^{N^{(2)}} \\der{L_t}{\\bm{U}}\\right|_{k,i,j}, \\ \\ i=1,2,\\dots,N^{(1)}, \\ \\ j=1,2,\\dots,n \\\\\n\\end{align}\n\\]\nHere, \\(\\alpha\\) is the learning rate.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Updating parameters and RNN training</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html#computing-loss-gradients-of-layer-2-parameters",
    "href": "weightUpdate.html#computing-loss-gradients-of-layer-2-parameters",
    "title": "4  Back Propagation Through Time",
    "section": "",
    "text": "Note\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{c}}\\) is a matrix of dimension \\(N^{(2)}\\times N^{(2)}\\), with 1st dimension being number of components in the loss term.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{V}}\\) is a 3D tensor of dimension \\(N^{(2)}\\times N^{(2)} \\times N^{(1)}\\), with 1st dimension being number of components in the loss term.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html#computing-loss-gradients-of-layer-1-parameters",
    "href": "weightUpdate.html#computing-loss-gradients-of-layer-1-parameters",
    "title": "4  Back Propagation Through Time",
    "section": "4.2 Computing loss gradients of layer 1 parameters",
    "text": "4.2 Computing loss gradients of layer 1 parameters\nTo start with layer 1 parameters, we need a couple of intermediate derivatives that are computed below.\n\\[\n\\begin{align}\n\\frac{\\partial L_t}{\\partial \\textbf{a}_t} &= \\frac{\\partial L_t}{\\partial \\textbf{o}_t} \\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{a}_t} \\\\\n&= \\delta_t^{(2)}\n\\begin{bmatrix}\n\\frac{\\partial o_{1,t}}{\\partial a_{1,t}} & \\frac{\\partial o_{1,t}}{\\partial a_{2,t}} & \\frac{\\partial o_{1,t}}{\\partial a_{3,t}} \\\\\n\\frac{\\partial o_{2,t}}{\\partial a_{1,t}} & \\frac{\\partial o_{2,t}}{\\partial a_{2,t}} & \\frac{\\partial o_{2,t}}{\\partial a_{3,t}} \\\\\n\\end{bmatrix} \\\\\n&= \\delta_t^{(2)}\n\\begin{bmatrix}\nv_{1,1} & v_{1,2} & v_{1,3} \\\\\nv_{2,1} & v_{2,2} & v_{3,3} \\\\\n\\end{bmatrix} \\\\\n&= \\delta_T^{(2)} \\textbf{V}\n\\end{align}\n\\tag{4.7}\\]\nAnd\n\\[\n\\begin{align}\n\\frac{\\partial L_t}{\\partial \\textbf{z}_t} &= \\frac{\\partial L_t}{\\partial \\textbf{a}_t} \\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{z}_t} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t}\n\\begin{bmatrix}\n\\frac{\\partial a_{1,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{1,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{1,t}}{\\partial z_{3,t}} \\\\\n\\frac{\\partial a_{2,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{2,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{2,t}}{\\partial z_{3,t}} \\\\\n\\frac{\\partial a_{3,t}}{\\partial z_{1,t}} & \\frac{\\partial a_{3,t}}{\\partial z_{2,t}} & \\frac{\\partial a_{3,t}}{\\partial z_{3,t}} \\\\\n\\end{bmatrix} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t}\n\\begin{bmatrix}\nh'(z_{1,t}) & 0 & 0 \\\\\n0 & h'(z_{1,t}) & 0 \\\\\n0 & 0 & h'(z_{1,t}) \\\\\n\\end{bmatrix} \\\\\n&= \\frac{\\partial L_t}{\\partial \\textbf{a}_t} \\diag{h'\\left(\\textbf{z}_t\\right)} \\\\\n&= \\delta_t^{(1)}\n\\end{align}\n\\tag{4.8}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\delta_t^{(1)}\\) is a matrix of size \\(N^{(2)}\\times N^{(1)}\\).\n\n\nNow, we will start with the \\(\\textbf{b}\\) parameter’s loss gradient.\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{b}} = \\frac{\\partial L_t}{\\partial \\textbf{z}_t} \\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}}\n\\tag{4.9}\\]\nIt is evident from Equation 2.2 and Equation 2.3 that \\(\\textbf{a}_t\\) is dependent on \\(\\textbf{z}_t\\). However, \\(\\textbf{z}_t\\) is dependent upon \\(\\textbf{a}_{t-1}\\), and the chain goes on till \\(t=0\\). Let me call this as sequence chain link. Because of this, the \\(\\textbf{z}_t\\) derivative in above equation will be written as \\[\n\\begin{align}\n\\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}} &= \\frac{\\partial \\textbf{b}}{\\partial \\textbf{b}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}} \\\\\n&= \\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}} \\\\\n\\end{align}\n\\]\nWe will keep \\(\\partial\\textbf{a}_{t-1}/\\partial\\textbf{b}\\) as it is for now. Substituting above expression in Equation 4.9 gives\n\\[\n\\frac{\\partial L_t}{\\partial \\textbf{b}} = \\delta_t^{(1)} \\left[\\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}}\\right]. \\\\\n\\tag{4.10}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\frac{\\partial L_t}{\\partial \\textbf{b}}\\) is a vector of shape \\(N^{(2)}\\times N^{(1)}\\).\n\n\nNext, we compute \\[\n\\begin{align}\n\\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{b}} &= \\frac{\\partial \\textbf{a}_t}{\\partial \\textbf{z}_t} \\frac{\\partial \\textbf{z}_t}{\\partial \\textbf{b}} \\\\\n&= \\diag{h'(\\textbf{z}_t)} \\textbf{I}_{N^{(1)}} \\\\\n&= \\diag{h'(\\textbf{z}_t)}\n\\end{align}\n\\tag{4.11}\\]\nwhich is a \\(N^{(1)}\\times N^{(1)}\\) matrix.\nNow, we move to compute \\(\\textbf{W}\\) parameter’s loss gradient. From Equation 2.2, \\[\n\\renewcommand{\\bm}[1]{\\textbf{#1}}\n\\renewcommand{\\der}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\] \\[\n\\begin{align}\n\\der{L_t}{\\bm{W}} &= \\der{L_t}{\\bm{z}_t}\\der{\\bm{z}_t}{\\bm{W}} \\\\\n&= \\delta_t^{(1)}\\left[\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}} + \\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{W}}\\right] \\\\\n&= \\delta_t^{(1)} \\omega_1.\n\\end{align}\n\\tag{4.12}\\]\nLets compute each term in \\(\\omega_1\\) in above equation. Let \\[\n\\lambda = \\bm{W} \\bm{a}_{t-1} =\n\\begin{bmatrix}\nw_{1,1} a_{1,t-1} + w_{1,2}a_{2,t-1} + w_{1,3} a_{3,t-1} \\\\\nw_{2,1} a_{1,t-1} + w_{2,2}a_{2,t-1} + w_{2,3} a_{3,t-1} \\\\\nw_{3,1} a_{1,t-1} + w_{3,2}a_{2,t-1} + w_{3,3} a_{3,t-1} \\\\\n\\end{bmatrix}\n\\]\nNow, \\[\n\\begin{align}\n\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}} = \\der{\\lambda}{\\bm{W}} &=\n\\left[\\left[\\der{\\lambda_1}{\\bm{W}}\\right]\\left[\\der{\\lambda_2}{\\bm{W}}\\right]\\left[\\der{\\lambda_3}{\\bm{W}}\\right]\\right] \\\\\n&= \\left[\n\\begin{bmatrix}\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\na_{1,t-1} & a_{2,t-1} & a_{3,t-1} \\\\\n\\end{bmatrix}\n\\right] \\\\\n\\der{\\bm{W}\\bm{a}_{t-1}}{\\bm{W}}&= \\reshape{\\bm{a}_{t-1}^T\\otimes \\bm{I}_{N^{(1)}}}{N^{(1)}\\times N^{(1)}\\times N^{(1)}} .\n\\end{align}\n\\tag{4.13}\\]\nWe keep \\(\\der{\\bm{a}_{t-1}}{\\bm{W}}\\) as it is for now in Equation 4.12 like previously did. Then,\n\\[\n\\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{W}} = \\reshape{\\bm{W}\\  \\reshape{\\der{\\bm{a}_{t-1}}{\\bm{W}}}{N^{(1)}\\times (N^{(1)}* N^{(1)})}}{N^{(1)}\\times N^{(1)}\\times N^{(1)}}.\n\\tag{4.14}\\]\nSubstituting Equation 4.13 and Equation 4.14 in Equation 4.12 can compute \\(\\omega_1\\) term. Thus, \\[\n\\der{L_t}{\\bm{W}} = \\reshape{\\delta_t^{(1)} \\ \\reshape{\\omega_1}{N^{(1)}\\times (N^{(1)}*N^{(1)})}}{N^{(2)}\\times N^{(1)} \\times N^{(1)}}.\n\\tag{4.15}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\der{L_t}{\\bm{W}}\\) is a 3D tensor of shape \\(N^{(2)}\\times N^{(1)} \\times N^{(1)}\\).\n\n\nNow, computing the last parameter \\(\\bm{U}\\) derivative of loss function\n\\[\n\\begin{align}\n\\der{L_t}{\\bm{U}} &= \\der{L_t}{\\bm{z}_t}\\der{\\bm{z}_t}{\\bm{U}} \\\\\n&= \\delta_t^{(1)} \\left[\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} + \\bm{W} \\der{\\bm{a}_{t-1}}{\\bm{U}}\\right]\\\\\n&= \\delta_t^{(2)} \\omega_2.\n\\end{align}\n\\tag{4.16}\\]\nNow to compute \\(\\omega_2\\) in above equation. Let \\[\n\\psi = \\bm{U}\\bm{x}_t =\n\\begin{bmatrix}\nu_{1,1} x_{1,t} + u_{1,2} x_{2,t} \\\\\nu_{2,1} x_{1,t} + u_{2,2} x_{2,t} \\\\\nu_{3,1} x_{1,t} + u_{3,2} x_{2,t} \\\\\n\\end{bmatrix}.\n\\]\nThen\n\\[\n\\begin{align}\n\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} = \\der{\\psi}{\\bm{U}} &=\n\\left[\\left[\\der{\\psi_1}{\\bm{U}}\\right]\\left[\\der{\\psi_2}{\\bm{U}}\\right]\\left[\\der{\\psi_3}{\\bm{U}}\\right]\\right] \\\\\n&= \\left[\n\\begin{bmatrix}\nx_{1,t} & x_{2,t} \\\\\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\nx_{1,t} & x_{2,t} \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\nx_{1,t} & x_{2,t} \\\\\n\\end{bmatrix}\n\\right] \\\\\n\\der{\\bm{U}\\bm{x}_t}{\\bm{U}} &= \\reshape{\\bm{x}_t^T \\otimes \\bm{I}_{N^{(1)}}}{N^{(1)}\\times N^{(1)} \\times n} .\n\\end{align}\n\\tag{4.17}\\]\nNow, in the other term, we will keep \\(\\der{\\bm{a}_{t-1}}{\\bm{U}}\\) as it is for now like previously did. Then, \\[\n\\bm{W}\\der{\\bm{a}_{t-1}}{\\bm{U}} = \\reshape{\\bm{W} \\reshape{\\der{\\bm{a}_{t-1}}{\\bm{U}}}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(1)}\\times N^{(1)}\\times n}\n\\tag{4.18}\\]\nNow, by substituting Equation 4.17 and Equation 4.18 in Equation 4.16, we can compute \\(\\omega_2\\). Thus,\n\\[\n\\der{L_t}{\\bm{U}} = \\reshape{\\delta_t^{(1)} \\reshape{\\omega_2}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(2)}\\times N^{(1)} \\times n}\n\\tag{4.19}\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\der{L_t}{\\bm{U}}\\) is a 3D tensor of size \\(N^{(2)}\\times N^{(1)} \\times n\\).\n\n\nThis concludes the loss equation derivatives computation for all model parameters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html#summary-of-loss-gradients",
    "href": "weightUpdate.html#summary-of-loss-gradients",
    "title": "4  Back Propagation Through Time",
    "section": "4.3 Summary of loss gradients",
    "text": "4.3 Summary of loss gradients\nBelow is the summary of the loss gradient equations for all parameters.\n\\[\n\\begin{align}\n\\der{L_t}{\\bm{c}} &= \\delta_t^{(2)}\\\\\n\\frac{\\partial L_t}{\\partial \\textbf{V}} &= \\reshape{\\delta_t^{(2)} \\ \\reshape{\\frac{\\partial \\textbf{o}_t}{\\partial \\textbf{V}}}{N^{(2)}\\times (N^{(2)}* N^{(1)})}}{ N^{(2)}\\times N^{(2)}\\times N^{(1)}} \\\\\n\\frac{\\partial L_t}{\\partial \\textbf{b}} &= \\delta_t^{(1)} \\left[\\textbf{I}_{N^{(1)}} + \\textbf{W}\\frac{\\partial \\textbf{a}_{t-1}}{\\partial \\textbf{b}}\\right] \\\\\n\\der{L_t}{\\bm{W}} &= \\reshape{\\delta_t^{(1)} \\ \\reshape{\\omega_1}{N^{(1)}\\times (N^{(1)}*N^{(1)})}}{N^{(2)}\\times N^{(1)} \\times N^{(1)}} \\\\\n\\der{L_t}{\\bm{U}} &= \\reshape{\\delta_t^{(1)} \\reshape{\\omega_2}{N^{(1)}\\times (N^{(1)}*n)}}{N^{(2)}\\times N^{(1)} \\times n}\n\\end{align}\n\\]\nIn the next section, we will discuss about updating model parameters and the pseudocode for RNN training.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Back Propagation Through Time</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html#updating-parameters",
    "href": "weightUpdate.html#updating-parameters",
    "title": "4  Updating parameters and RNN training",
    "section": "",
    "text": "Note\n\n\n\nThe above equations are updating parameters based on one sub-part of data only! Hence, the algorithms like Batch Gradient Descent and their variants (refer here) will involve additional summation over the batch of data being used to update the parameters in the above equations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Updating parameters and RNN training</span>"
    ]
  },
  {
    "objectID": "weightUpdate.html#training-rnns",
    "href": "weightUpdate.html#training-rnns",
    "title": "4  Updating parameters and RNN training",
    "section": "4.2 Training RNNs",
    "text": "4.2 Training RNNs\nWe now have the loss gradients equations in hand. In this section we will see on how to use all these derived equations to Train an RNN.\nFirst, the data will be loaded and then encoding will be performed. Here, encoding will be needed as the input data will be a word (a sequence of characters). Encoding may be skipped for numerical data sequence.\nThe size of input and output vectors will be determined during the encoding step. Thus, the number of hidden neurons will be have to be chosen along with learning rate.\nWe have to have a set of hidden vector and its derivatives at \\(t=0\\) for the algorithm to work. Hence we have initialized the hidden vector and tensor variables with zeros. Then we have to initialize the network parameters with appropriate techniques such as Xavier Initialization.\nThen, the training loop starts. We will perform forward propagation to get the initial prediction of output vector \\(\\hat{\\bm{y}}\\) which is then used to compute loss value. Then, the \\(\\hat{\\bm{y}}\\) will be taken to back-propagation through time along with other network parameters for loss gradients computation. Finally, the network parameters will be updated with the computed loss gradients.\nThe training loop will be continued until the model converges. The usual convergence criteria will be to set a threshold on loss value and the model is taken as converged/trained when loss goes below threshold.\nThe algorithm for RNN training is given below (click on the image for enlarged view). \nEach main step in the above algorithm is defined as a function. The forward propagation function is given below. \nHere, the decoding step in the end of forward propagation is optional and will be used only during model inference. Hence, that step can be skipped.\nThen the control in algorithm then goes to back-propagation through time function which is defined below. \nHere, the loss parameter gradient equations are given in the order of execution so that the dependency of variables is satisfied.\nOnce back-propagation is done and the gradients are available, the control then moves to the parameter update function that is defined below. \nAnd then, the loop continues until the convergence criteria is met. This concludes the RNN training algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Updating parameters and RNN training</span>"
    ]
  }
]